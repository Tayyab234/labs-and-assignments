{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset (assuming your dataset is stored in a variable named 'data')\n",
    "# Replace 'data' with the actual variable or file path you have\n",
    "\n",
    "# Data Analysis & Visualization Lab\n",
    "\n",
    "# 2. Data Preprocessing & Training\n",
    "\n",
    "# a) Tokenize Text\n",
    "# i. Convert in sentences\n",
    "sentences = [sent_tokenize(comment) for comment in data['body']]\n",
    "\n",
    "# ii. Convert in words\n",
    "words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# b) Remove infrequent words\n",
    "# i. Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [[word for word in sentence if word.lower() not in stop_words] for sentence in words]\n",
    "\n",
    "# c) Build training & test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_words, data['target_column'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Building the RNN\n",
    "\n",
    "# a) Initialize Assistance parameters\n",
    "word_dim = 100  # replace with your desired dimension\n",
    "hidden_dim = 50  # replace with your desired dimension\n",
    "output_dim = 1  # replace with your desired dimension\n",
    "bptt_truncate = 5  # replace with your desired value\n",
    "\n",
    "# b) Initialize Network parameters\n",
    "U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, word_dim))\n",
    "V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (output_dim, hidden_dim))\n",
    "W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))\n",
    "\n",
    "# c) Activate Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# d) Forward_Propagation\n",
    "def forward_propagation(x):\n",
    "    # Perform forward pass to get predictions\n",
    "    # Replace this with your specific forward pass logic\n",
    "\n",
    "    # Predict the highest score\n",
    "    # Replace this with your logic to get the highest score\n",
    "\n",
    "# e) Calculate loss\n",
    "def calculate_loss(predictions, targets):\n",
    "    # Create a loss function to measure errors\n",
    "    # Replace this with your specific loss function logic\n",
    "\n",
    "# Note: Evaluating loss depends on the size of the dataset, so try to minimize it by random.\n",
    "# Perform operations on the given data and update the network parameters accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "example = 'This is a short sentence (1) with one reference to an image. This next sentence, while non-sensical, does not have an image and has two commas.'\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts([example])\n",
    "s = tokenizer.texts_to_sequences([example])[0]\n",
    "' '.join(tokenizer.index_word[i] for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(texts,\n",
    "                   training_length=50,\n",
    "                   lower=True,\n",
    "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
    "\n",
    "    # Create the tokenizer object and train on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Create look-up dictionaries and reverse look-ups\n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words = len(word_idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "\n",
    "    print(f'There are {num_words} unique words.')\n",
    "\n",
    "    # Convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    over_idx = [\n",
    "        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n",
    "    ]\n",
    "\n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "\n",
    "    # Only keep sequences with more than training length tokens\n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])\n",
    "\n",
    "    training_seq = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through the sequences of tokens\n",
    "    for seq in new_sequences:\n",
    "\n",
    "        # Create multiple training examples from each sequence\n",
    "        for i in range(training_length, len(seq)):\n",
    "            # Extract the features and label\n",
    "            extract = seq[i - training_length:i + 1]\n",
    "\n",
    "            # Set the features and label\n",
    "            training_seq.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "\n",
    "    print(f'There are {len(training_seq)} training sequences.')\n",
    "\n",
    "    # Return everything needed for setting up the model\n",
    "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, training_seq, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_LEGNTH = 50\n",
    "filters = '!\"#$%&()*+/:<=>@[\\\\]^_`{|}~\\t\\n'\n",
    "word_idx, idx_word, num_words, word_counts, abstracts, sequences, features, labels = make_sequences(\n",
    "    formatted, TRAINING_LENGTH, lower=True, filters=filters)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
