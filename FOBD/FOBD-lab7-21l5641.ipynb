{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lab NO 7\n",
    "from pyspark import SparkContext, SparkConf\n",
    "appName='lab7'\n",
    "master=\"local[*]\"\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lab tast no 1\n",
    "list1=[1,2,3,4,5,6,7,8,9,10]\n",
    "list2=list(map(lambda x : x**2,list1))\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tast no 2\n",
    "#Create my_list_2 which contains 20 random numbers. Print the list. Filter the numbers divisible by 5\n",
    "#from my_list2 using filter() and lambda().Print the numbers divisible by 5 from my_list2.\n",
    "\n",
    "import random\n",
    "list2 = [random.randint(1, 100) for _ in range(20)]\n",
    "print(list2)\n",
    "filtered_list = list(filter(lambda x: x % 5 == 0, list2))\n",
    "print(\"Numbers divisible by 5 from my_list_2:\", filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task no 3\n",
    "#Create an RDD named RDD from a list of words which is created by yourself. Confirm the object created is RDD.\n",
    "\n",
    "\n",
    "RDD = spark.sparkcontext.parallelize([\"Hello\", \"world\",\"pyspark\",\"object\",\"created\"])\n",
    "RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task no 4\n",
    "#Create an RDD named fileRDD from a given input file. Print the type of the fileRDD created.\n",
    "\n",
    "fileRDD = sc.textFile('inputfile.txt')\n",
    "\n",
    "print(\"Type of fileRDD:\", type(fileRDD))\n",
    "\n",
    "# Display the first few lines of the RDD\n",
    "for line in fileRDD.take(5):\n",
    "    print(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task no 5\n",
    "#Find the number of partitions that support fileRDD RDD. Create an RDD named fileRDD_part from\n",
    "#the input file but create 5 partitions. Confirm the number of partitions in the new fileRDD_part RDD.\n",
    "\n",
    "# Find the number of partitions in the original fileRDD\n",
    "num_partitions_original = fileRDD.getNumPartitions()\n",
    "print(\"Number of partitions in fileRDD:\", num_partitions_original)\n",
    "fileRDD_part = fileRDD.repartition(3)  \n",
    "\n",
    "#Confirm the number of partitions in the new fileRDD_part\n",
    "num_partitions_new = fileRDD_part.getNumPartitions()\n",
    "print(\"Number of partitions in fileRDD_part:\", num_partitions_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 6\n",
    "RDDnum=sc.parallelize([1,2,3,4,5,6])\n",
    "RDDresult=RDDnum.map(lambda x : (x,x**3))\n",
    "RDDresult.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 7\n",
    "fileRDD1 = sc.textFile('inputfile.txt')\n",
    "fileRDDresult=fileRDD1.filter(lambda x : 'beautiful' in x)\n",
    "fileRDDresult.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 8\n",
    "list=['hello world','read text in string']\n",
    "rdd1=sc.parallelize(list)\n",
    "rdd2=rdd1.flatMap(lambda x : x.split(' '))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 9\n",
    "import random\n",
    "list1=[random.randint(1,100) for _ in range(0,10)]\n",
    "print(list1)\n",
    "l1=max(list1)\n",
    "rdd1=sc.parallelize(list1)\n",
    "rdd2=rdd1.filter(lambda x: x==l1)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 10\n",
    "list1=[1,2,3,5,6,7,8]\n",
    "list2=[2,9,5,4,11,23,56]\n",
    "rdd1=sc.parallelize(list1)\n",
    "rdd2=sc.parallelize(list2)\n",
    "matching_elements = rdd1.intersection(rdd2)\n",
    "\n",
    "matching_elements_list = matching_elements.collect()\n",
    "print(\"Matching elements of the lists:\", matching_elements_list)\n",
    "\n",
    "# Stop the Spark context\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = sc.parallelize(data, 3)  # Creating an RDD with 3 partitions\n",
    "rdd.collect()\n",
    "rdd.getNumPartitions()\n",
    "\n",
    "def add_constant(iterator):\n",
    "    constant = 10\n",
    "    yield [x + constant for x in iterator]\n",
    "\n",
    "# Using mapPartitions to add a constant value to elements in each partition\n",
    "result = rdd.mapPartitions(add_constant)\n",
    "\n",
    "#result=rdd.map(lambda x : x+10)\n",
    "# Collect and print the result\n",
    "print(result.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
